{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization ,Activation , MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras import layers as ls\n",
    "from keras.models import Model \n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import concatenate\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "x = base_model.output\n",
    "x = ls.Flatten()(x)\n",
    "x = Dense(28, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "\n",
    "for layer in model.layers[0:20]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "    \n",
    "early = EarlyStopping(monitor = 'val_accuracy', min_delta = 0, \n",
    "                      patience = 10, verbose= 1 , mode = 'auto')\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.load_weights(\"arsl-2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 128, 128, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 128, 128, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 64, 64, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 64, 64, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 32, 32, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 32, 32, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 32, 32, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 16, 16, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 28)                229404    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,944,092\n",
      "Trainable params: 229,404\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\VS Code\\Grad k7yan\\main.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/VS%20Code/Grad%20k7yan/main.ipynb#ch0000005?line=50'>51</a>\u001b[0m image\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/VS%20Code/Grad%20k7yan/main.ipynb#ch0000005?line=51'>52</a>\u001b[0m \u001b[39m# Detections\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/VS%20Code/Grad%20k7yan/main.ipynb#ch0000005?line=52'>53</a>\u001b[0m results \u001b[39m=\u001b[39m hands\u001b[39m.\u001b[39;49mprocess(image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/VS%20Code/Grad%20k7yan/main.ipynb#ch0000005?line=53'>54</a>\u001b[0m \u001b[39m# Set flag to true\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/VS%20Code/Grad%20k7yan/main.ipynb#ch0000005?line=54'>55</a>\u001b[0m image\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MohaMedFRy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, image: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NamedTuple:\n\u001b[0;32m    133\u001b[0m   \u001b[39m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[39m         right hand) of the detected hand.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mprocess(input_data\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m: image})\n",
      "File \u001b[1;32mc:\\Users\\MohaMedFRy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\python\\solution_base.py:350\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    344\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    345\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    346\u001b[0m         stream\u001b[39m=\u001b[39mstream_name,\n\u001b[0;32m    347\u001b[0m         packet\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    348\u001b[0m                                  data)\u001b[39m.\u001b[39mat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 350\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49mwait_until_idle()\n\u001b[0;32m    351\u001b[0m \u001b[39m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[39m# output stream names.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m solution_outputs \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mnamedtuple(\n\u001b[0;32m    354\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSolutionOutputs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_stream_type_info\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "import operator\n",
    "import cv2\n",
    "import sys, os\n",
    "from keras.models import load_model\n",
    "import mediapipe as mp\n",
    "import uuid\n",
    "\n",
    "\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "#================================================\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "#================================================\n",
    "output_Phrase=\"\"\n",
    "\n",
    "\n",
    "count_of_writen_letter=0\n",
    "l=[]\n",
    "numframe=0\n",
    "count=0\n",
    "arabic_imge_width=800\n",
    "arabic_imge_hight=400\n",
    "how_many_count_to_write_the_letter=2\n",
    "how_many_count_frame_to_predict=20\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3,3000)\n",
    "cap.set(4,3000)\n",
    "w=cap.get(3)\n",
    "h=cap.get(4)\n",
    "\n",
    "# print(w)\n",
    "# print(h)\n",
    "\n",
    "sign_image=np.ones((1500,1500))\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands: \n",
    "    while cap.isOpened():\n",
    "\n",
    "        start=time.time()\n",
    "        ret, frame = cap.read()\n",
    "        # BGR 2 RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # Flip on horizontal\n",
    "        image = cv2.flip(image, 1)\n",
    "        # Set flag\n",
    "        image.flags.writeable = False\n",
    "        # Detections\n",
    "        results = hands.process(image)\n",
    "        # Set flag to true\n",
    "        image.flags.writeable = True\n",
    "        # RGB 2 BGR\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        # Rendering results\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand in results.multi_hand_landmarks:\n",
    "                xl=[]\n",
    "                yl=[]\n",
    "                zl=[]\n",
    "                for point in hand.landmark:\n",
    "                    xl.append(point.x)\n",
    "                    yl.append(point.y)\n",
    "                    zl.append(point.z)\n",
    "                x1=int(min(xl)*w)-80\n",
    "                y1=int(min(yl)*h)-50\n",
    "                x2=int(max(xl)*w)+50\n",
    "                y2=int(max(yl)*h)+50\n",
    "                if x1<0:\n",
    "                    x1=0\n",
    "                if y1<0:\n",
    "                    y1=0\n",
    "\n",
    "            cv2.rectangle(image,(x1,y1),(x2,y2),(0,0,255),1)\n",
    "            #sign_image\n",
    "            sign_image=image[y1:y2,x1:x2]\n",
    "            try:\n",
    "                   \n",
    "                if numframe%how_many_count_frame_to_predict==0:\n",
    "    \n",
    "                    sign_image = cv2.resize(sign_image, (128, 128))               \n",
    "                    sign_image=sign_image.reshape(1, 128, 128, 3)\n",
    "                    result = model.predict(sign_image)\n",
    "\n",
    "                    #English mapping\n",
    "                    #=========================================================================\n",
    "                    prediction1 = { '7a2'  : result[0][0],\n",
    "                                    'ain'  : result[0][1],\n",
    "                                    'alef' : result[0][2],\n",
    "                                    'ba2'  : result[0][3],\n",
    "                                    'daal' : result[0][4],\n",
    "                                    'dad'  : result[0][5],\n",
    "                                    'faa'  : result[0][6],\n",
    "                                    'gem'  : result[0][7],\n",
    "                                    'ghain': result[0][8],\n",
    "                                    'haa'  : result[0][9],\n",
    "                                    'kaaf' : result[0][10],\n",
    "                                    'kha'  : result[0][11],\n",
    "                                    'laam' : result[0][12],\n",
    "                                    'meem' : result[0][13],\n",
    "                                    'noon' : result[0][14],\n",
    "                                    'qaaf' : result[0][15],\n",
    "                                    'ra2'  : result[0][16],\n",
    "                                    'sad'  : result[0][17],\n",
    "                                    'seen' : result[0][18],\n",
    "                                    'sheen': result[0][19],\n",
    "                                    'ta2'  : result[0][20],\n",
    "                                    'tah'  : result[0][21],\n",
    "                                    'tha2' : result[0][22],\n",
    "                                    'waaw' : result[0][23],\n",
    "                                    'yaa'  : result[0][24],\n",
    "                                    'zaaa' : result[0][25],\n",
    "                                    'zain' : result[0][26],\n",
    "                                    'zal'  : result[0][27]\n",
    "                    }\n",
    "                    prediction2 = { 'ح'  : result[0][0],\n",
    "                                    'ع'  : result[0][1],\n",
    "                                    'ا' : result[0][2],\n",
    "                                    'ب'  : result[0][3],\n",
    "                                    'د' : result[0][4],\n",
    "                                    'ض'  : result[0][5],\n",
    "                                    'ف'  : result[0][6],\n",
    "                                    'ج'  : result[0][7],\n",
    "                                    'غ': result[0][8],\n",
    "                                    'ه'  : result[0][9],\n",
    "                                    'ك' : result[0][10],\n",
    "                                    'خ'  : result[0][11],\n",
    "                                    'ل' : result[0][12],\n",
    "                                    'م' : result[0][13],\n",
    "                                    'ن' : result[0][14],\n",
    "                                    'ق' : result[0][15],\n",
    "                                    'ر'  : result[0][16],\n",
    "                                    'ص'  : result[0][17],\n",
    "                                    'س' : result[0][18],\n",
    "                                    'ش': result[0][19],\n",
    "                                    'ت'  : result[0][20],\n",
    "                                    'ط'  : result[0][21],\n",
    "                                    'ث' : result[0][22],\n",
    "                                    'و' : result[0][23],\n",
    "                                    'ي'  : result[0][24],\n",
    "                                    'ظ' : result[0][25],\n",
    "                                    'ز' : result[0][26],\n",
    "                                    'ذ'  : result[0][27]\n",
    "                    }\n",
    "                    # Sorting based on top prediction\n",
    "                    #=========================================================================\n",
    "                    prediction1 = sorted(prediction1.items(), key=operator.itemgetter(1), reverse=True)\n",
    "                    prediction2 = sorted(prediction2.items(), key=operator.itemgetter(1), reverse=True)\n",
    "                    # Sorting based on top prediction\n",
    "                    #append the list\n",
    "                               \n",
    "\n",
    "                    if(len(l)==0):\n",
    "                        l.append(prediction1[0][0])\n",
    "                        lcount=count\n",
    "                    if prediction1[0][0]==l[-1]:\n",
    "                        count=count+1\n",
    "                    if count==how_many_count_to_write_the_letter:\n",
    "                        output_Phrase=output_Phrase+prediction2[0][0]\n",
    "                        count_of_writen_letter=count_of_writen_letter+1\n",
    "                        count=0\n",
    "                        l=[]\n",
    "                    if count == lcount:\n",
    "                        count=0\n",
    "                    l.append(prediction1[0][0]) \n",
    "\n",
    "                cv2.putText(image, \"{}\".format(count_of_writen_letter),(550,300), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,0,255), 2)\n",
    "                cv2.putText(image, prediction1[0][0],(500,400), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,0,255), 2) \n",
    "                cv2.imshow('Hand Tracking', image)\n",
    "#                 print(\"frame\")\n",
    "#                 print(numframe)\n",
    "                numframe=numframe+1\n",
    "                #show the line of all letter\n",
    "                #=========================================================================\n",
    "                reshaped_text = arabic_reshaper.reshape(output_Phrase)\n",
    "                bidi_text = get_display(reshaped_text) \n",
    "                fontpath = \"arial.ttf\"\n",
    "                font = ImageFont.truetype(fontpath, 32)\n",
    "            \n",
    "\n",
    "                img=np.full([arabic_imge_hight,arabic_imge_width,3],0,np.uint8)\n",
    "                cv2.rectangle(img,(0,0),(arabic_imge_width,arabic_imge_hight),(50,180,30),-1)\n",
    "                img_pil = Image.fromarray(img)\n",
    "                draw = ImageDraw.Draw(img_pil)\n",
    "                draw.text((400,180),bidi_text, font = font)\n",
    "                img = np.array(img_pil)              \n",
    "                cv2.imshow('arabic image',img)\n",
    "                \n",
    "\n",
    "            except ValueError:\n",
    "                print(ValueError)\n",
    "                print(\"Error form predict area\")\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(output_Phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "159bae849a68ffa568efbbba0589828887277fc7b3e55c6682696f3c54d15256"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
